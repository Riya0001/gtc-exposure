{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual HRNet For Localisation and Damage Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is base on:\n",
    "\n",
    "###### HRNet-Semantic-Segmentation (https://github.com/HRNet/HRNet-Semantic-Segmentation)\n",
    "###### Modified code based Copyright (c) Microsoft\n",
    "###### Licensed under the MIT License.\n",
    "###### Written by Ke Sun (sunk@mail.ustc.edu.cn)\n",
    "\n",
    "\n",
    "And the paper: \n",
    "###### Dual-HRNet for Building Localization and Damage Classification Jamyoung Koo, Junghoon Seo, Kwangjin Yoon, Taegyun Jeon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lovasz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d5e1bcc195e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0myacs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCfgNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlovasz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlovasz_softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual_hrnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAverageMeter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lovasz'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import argparse\n",
    "import multiprocessing\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from yacs.config import CfgNode\n",
    "\n",
    "from lovasz import lovasz_softmax\n",
    "from models.dual_hrnet import get_model\n",
    "from utils import AverageMeter, adjust_learning_rate\n",
    "from xview2 import XView2Dataset\n",
    "from utils import safe_mkdir\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, required=True, default=\"\",\n",
    "                    help='path the data folder')\n",
    "parser.add_argument('--config_path', type=str, default=\"configs/dual-hrnet.yaml\",\n",
    "                    help='path of model config(ex:.yaml')\n",
    "parser.add_argument(\"--ckpt_save_dir\", type=str, default='ckpt/dual-hrnet/',\n",
    "                    help='path to save checkpoints')\n",
    "parser.add_argument('--test', type=str, required=False,\n",
    "                    help='testing the model')\n",
    "parser.add_argument(\"--local_rank\", type=int, default=0)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "safe_mkdir(args.ckpt_save_dir)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.addHandler(logging.FileHandler(os.path.join(args.ckpt_save_dir, 'train.log')))\n",
    "logger.setLevel(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "class ModelLossWraper(nn.Module):\n",
    "    def __init__(self, model, class_weights=None, is_disaster_perd=False, is_split_loss=True):\n",
    "        super(ModelLossWraper, self).__init__()\n",
    "        if class_weights is None:\n",
    "            class_weights = []\n",
    "\n",
    "        self.model = model.cuda()\n",
    "\n",
    "        self.criterion = lovasz_softmax\n",
    "\n",
    "        self.weights = class_weights\n",
    "        self.is_disaster_pred = is_disaster_perd\n",
    "        self.is_split_loss = is_split_loss\n",
    "\n",
    "    def forward(self, inputs_pre, inputs_post, targets, target_disaster):\n",
    "        inputs_pre = Variable(inputs_pre).cuda()\n",
    "        inputs_post = Variable(inputs_post).cuda()\n",
    "\n",
    "        pred_dict = self.model(inputs_pre, inputs_post)\n",
    "        loc = F.softmax(pred_dict['loc'], dim=1)\n",
    "        loc = F.interpolate(loc, size=targets.size()[1:3], mode='bilinear')\n",
    "\n",
    "        if self.is_split_loss:\n",
    "            cls = F.softmax(pred_dict['cls'], dim=1)\n",
    "            cls = F.interpolate(cls, size=targets.size()[1:3], mode='bilinear')\n",
    "\n",
    "            targets[targets == 255] = -1\n",
    "\n",
    "            loc_targets = targets.clone()\n",
    "            loc_targets[loc_targets > 0] = 1\n",
    "            loc_targets[loc_targets < 0] = 255\n",
    "            loc_targets = Variable(loc_targets).cuda()\n",
    "\n",
    "            cls_targets = targets.clone()\n",
    "            cls_targets = cls_targets - 1\n",
    "            cls_targets[cls_targets < 0] = 255\n",
    "            cls_targets = Variable(cls_targets).cuda()\n",
    "\n",
    "            # loss = self.criterion(outputs, targets, ignore_label=255)\n",
    "\n",
    "            loc_loss = self.criterion(loc, loc_targets, ignore=255)\n",
    "            cls_loss = self.criterion(cls, cls_targets, ignore=255, weights=self.weights)\n",
    "            total_loss = loc_loss + cls_loss\n",
    "        else:\n",
    "            targets = Variable(targets).cuda()\n",
    "            total_loss = self.criterion(loc, targets, ignore=255, weights=self.weights)\n",
    "\n",
    "        if self.is_disaster_pred:\n",
    "            target_disaster = Variable(target_disaster).cuda()\n",
    "            disaster_loss = F.cross_entropy(pred_dict['disaster'], target_disaster)\n",
    "            total_loss += disaster_loss * 0.05\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "def main():\n",
    "    if args.config_path:\n",
    "        with open(args.config_path, 'rb') as fp:\n",
    "            config = CfgNode.load_cfg(fp)\n",
    "    else:\n",
    "        config = None\n",
    "\n",
    "    ckpts_save_dir = args.ckpt_save_dir\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    test_model = None\n",
    "    max_epoch = config.TRAIN.NUM_EPOCHS\n",
    "    if 'test' in args:\n",
    "        test_model = args.test\n",
    "    print('data folder: ', args.data_dir)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # WORLD_SIZE Generated by torch.distributed.launch.py\n",
    "    num_gpus = int(os.environ[\"WORLD_SIZE\"]) if \"WORLD_SIZE\" in os.environ else 1\n",
    "    is_distributed = num_gpus > 1\n",
    "    if is_distributed:\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        torch.distributed.init_process_group(\n",
    "            backend=\"nccl\", init_method=\"env://\",\n",
    "        )\n",
    "\n",
    "    model = get_model(config)\n",
    "    model_loss = ModelLossWraper(model,\n",
    "                                 config.TRAIN.CLASS_WEIGHTS,\n",
    "                                 config.MODEL.IS_DISASTER_PRED,\n",
    "                                 config.MODEL.IS_SPLIT_LOSS,\n",
    "                                 )\n",
    "\n",
    "    if is_distributed:\n",
    "        model_loss = nn.SyncBatchNorm.convert_sync_batchnorm(model_loss)\n",
    "        model_loss = nn.parallel.DistributedDataParallel(\n",
    "            model_loss, device_ids=[args.local_rank], output_device=args.local_rank\n",
    "        )\n",
    "\n",
    "    trainset = XView2Dataset(args.data_dir, rgb_bgr='rgb',\n",
    "                             preprocessing={'flip': True,\n",
    "                                            'scale': config.TRAIN.MULTI_SCALE,\n",
    "                                            'crop': config.TRAIN.CROP_SIZE,\n",
    "                                            })\n",
    "\n",
    "    if is_distributed:\n",
    "        train_sampler = DistributedSampler(trainset)\n",
    "    else:\n",
    "        train_sampler = None\n",
    "\n",
    "    trainset_loader = torch.utils.data.DataLoader(trainset, batch_size=config.TRAIN.BATCH_SIZE_PER_GPU,\n",
    "                                                  shuffle=train_sampler is None, pin_memory=True, drop_last=True,\n",
    "                                                  sampler=train_sampler, num_workers=num_gpus)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    lr_init = config.TRAIN.LR\n",
    "    optimizer = torch.optim.SGD([{'params': filter(lambda p: p.requires_grad, model.parameters()), 'lr': lr_init}],\n",
    "                                lr=lr_init,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=0.,\n",
    "                                nesterov=False,\n",
    "                                )\n",
    "\n",
    "    start_epoch = 0\n",
    "    losses = AverageMeter()\n",
    "    model.train()\n",
    "    num_iters = max_epoch * len(trainset_loader)\n",
    "    for epoch in range(start_epoch, max_epoch):\n",
    "        if is_distributed:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "        cur_iters = epoch * len(trainset_loader)\n",
    "\n",
    "        for i, samples in enumerate(trainset_loader):\n",
    "            lr = adjust_learning_rate(optimizer, lr_init, num_iters, i + cur_iters)\n",
    "\n",
    "            inputs_pre = samples['pre_img']\n",
    "            inputs_post = samples['post_img']\n",
    "            target = samples['mask_img']\n",
    "            disaster_target = samples['disaster']\n",
    "\n",
    "            loss = model_loss(inputs_pre, inputs_post, target, disaster_target)\n",
    "\n",
    "            loss_sum = torch.sum(loss).detach().cpu()\n",
    "            if np.isnan(loss_sum) or np.isinf(loss_sum):\n",
    "                print('check')\n",
    "            losses.update(loss_sum, 4)  # batch size\n",
    "\n",
    "            loss = torch.sum(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if args.local_rank == 0 and i % 10 == 0:\n",
    "                logger.info('epoch: {0}\\t'\n",
    "                            'iter: {1}/{2}\\t'\n",
    "                            'lr: {3:.6f}\\t'\n",
    "                            'loss: {loss.val:.4f} ({loss.ema:.4f})'.format(\n",
    "                    epoch + 1, i + 1, len(trainset_loader), lr, loss=losses))\n",
    "\n",
    "        if args.local_rank == 0:\n",
    "            if (epoch + 1) % 50 == 0 and test_model is None:\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                }, os.path.join(ckpts_save_dir, 'hrnet_%s' % (epoch + 1)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    multiprocessing.set_start_method('spawn', True)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
