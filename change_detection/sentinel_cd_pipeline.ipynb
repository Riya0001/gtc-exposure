{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change detection for satellite imagery using weights trained on FCN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set variables and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "# Define data origin\n",
    "locName = 'ochosRios' # File name for area\n",
    "lat, long = 18.406, -77.103 # Lat/Long for center of area of interest\n",
    "mzoom = 13 # Zoom for interactive map\n",
    "newLocation = True # Will create new geojson and tiff files\n",
    "\n",
    "# Data labelling\n",
    "gjData = False # Location of gdb data (False if no labels, path to file if labels)\n",
    "dmgAss = \"/home/jovyan/gtc-exposure/change_detection/geojsons/\"+locName+\"Damage.geojson\" # Location of geojson (created if newLocation True)\n",
    "area, defArea = 0.0002, 0.025 # area is radius in lat/long around point label to be considered, defArea is for the case of no labels and defines area box size\n",
    "\n",
    "# Imagery variables\n",
    "imgColl, cloudFraction = \"sentinel-2:L1C\", 0.06 # Image collection and thrshold for cloud cover\n",
    "bandNum = 1 # 0-[r,g,b], 1-[r,g,b,nir], 2-[r,g,b,cloud-mask,red-edge-2,red-edge-3,red-edge-4,nir,swir1,swir2]\n",
    "imgNum = 2 # Number of image dates\n",
    "img_st, img_end = ['2017-08-15','2019-08-01'], ['2017-12-15','2019-12-01'] # Before and after image dates\n",
    "tifResolution, tifTilesize, tifPad = 10, 1024, 0 # True if first time on location, Size of images defined within AOI determined by geojson\n",
    "\n",
    "# Data storage\n",
    "dataPath = \"/home/jovyan/OSCD/new/\" # Path to save location for generated images\n",
    "\n",
    "# Model specifications\n",
    "LOAD_TRAINED = True # Load models (instead of re-training)\n",
    "newTest = True # False - Add to test files, True - Only test on current area\n",
    "TYPE = bandNum # Model type ~ band number\n",
    "modelWeights = '/home/jovyan/gtc-exposure/change_detection/net_final.pth.tar' # Weights file from best trained model (ask Seb for latest)\n",
    "PATH_TO_TRAIN = \"/home/jovyan/OSCD/onera/\" # Path to downloaded training data - will look to remove need for this\n",
    "FP_MODIFIER = 1 # Tuning parameter, use 1 if unsure\n",
    "PATCH_SIDE = 64\n",
    "BATCH_SIZE, NORMALISE_IMGS, TRAIN_STRIDE, DATA_AUG = 8, True, int(PATCH_SIDE/2) - 1, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tifffile imagecodecs\n",
    "\n",
    "# Python packages - if any are not installed use line above or \"pip install <package_name>\" in terminal\n",
    "import IPython\n",
    "import ipywidgets\n",
    "import ipyleaflet\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import geopandas as gpd\n",
    "import importlib\n",
    "import tifffile\n",
    "import imagecodecs\n",
    "from shutil import copyfile\n",
    "from skimage import io\n",
    "from tqdm import tqdm as tqdm\n",
    "from pandas import read_csv\n",
    "from math import floor, ceil, sqrt, exp\n",
    "import time\n",
    "from pprint import pprint\n",
    "import geojson\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "import descarteslabs as dl\n",
    "import descarteslabs.workflows as wf\n",
    "from descarteslabs.vectors import FeatureCollection, Feature, properties as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.6.0 . If below 1.6.0 you may need to restart kernel.\n"
     ]
    }
   ],
   "source": [
    "# import PyTorch and model functions\n",
    "#PyTorch\n",
    "from packaging import version\n",
    "import torch\n",
    "if version.parse(torch.__version__) < version.parse(\"1.6.0\"):\n",
    "    %pip install torch==1.6.0\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as tr\n",
    "\n",
    "# Models\n",
    "from models.unet import Unet\n",
    "# from siamunet_conc import SiamUnet_conc\n",
    "# from siamunet_diff import SiamUnet_diff\n",
    "# from fresunet import FresUNet\n",
    "# from smallunet import SmallUnet\n",
    "# from smallunet_attempt import Unet\n",
    "\n",
    "print('PyTorch version',torch.__version__,'. If below 1.6.0 you may need to restart kernel.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTS OK\n"
     ]
    }
   ],
   "source": [
    "# import custom functions from utils file\n",
    "from utils_cd import generate_tiff_from_polygons, save_test_results, test\n",
    "\n",
    "print('IMPORTS OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot time separated imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new geojson if necessary\n",
    "if newLocation and gjData:\n",
    "    g = gpd.read_file(gjData)\n",
    "    # Specify portion of database for new geojson file if desired (e.g. select event and lat/longs from FEMA dataset)\n",
    "    label = g.loc[((g['DMG_LEVEL'] == ('DES'))) & (g['EVENT_NAME'] == 'Hurricane Maria') & (g['LATITUDE'] > 18.4) & (g['LATITUDE'] < 18.45) & (g['LONGITUDE'] > -66.15) & (g['LONGITUDE'] < -65.95)]\n",
    "    \n",
    "    features = []\n",
    "    for i in label.index:\n",
    "        print(i, len(label.index))\n",
    "        poly = Polygon([[label.geometry.x[i], label.geometry.y[i]], [label.geometry.x[i]+area, label.geometry.y[i]], [label.geometry.x[i]+area, label.geometry.y[i]+area], [label.geometry.x[i], label.geometry.y[i]+area], [label.geometry.x[i], label.geometry.y[i]]])\n",
    "        features.append(geojson.Feature(properties={\"Damage\": label.DMG_LEVEL[i]}, geometry=poly))\n",
    "\n",
    "    fc = geojson.FeatureCollection(features)\n",
    "    with open(dmgAss, 'w') as f:\n",
    "        geojson.dump(fc, f)\n",
    "        \n",
    "elif newLocation:\n",
    "    features = []\n",
    "    poly = Polygon([[long-defArea, lat-defArea], [long+defArea, lat-defArea], [long+defArea, lat+defArea], [long-defArea, lat+defArea], [long-defArea, lat-defArea]])\n",
    "    features.append(geojson.Feature(properties={},geometry=poly))\n",
    "    fc = geojson.FeatureCollection(features)\n",
    "    with open(dmgAss, 'w') as f:\n",
    "        geojson.dump(fc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622c8ca2002e4043bc9d7ae57540e864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "\n",
       "`ipyleaflet` and/or `ipywidgets` Jupyter extensions are not installed! (or you're not in a Jupyter notebook.)\n",
       "To install for JupyterLab, run this in a cell:\n",
       "    !jupyter labextension install jupyter-leaflet @jupyter-widgets/jupyterlab-manager\n",
       "To install for plain Jupyter Notebook, run this in a cell:\n",
       "    !jupyter nbextension enable --py --sys-prefix ipyleaflet\n",
       "Then, restart the kernel and refresh the webpage.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = wf.interactive.MapApp()\n",
    "m.center = (lat, long)\n",
    "m.zoom = mzoom\n",
    "types = [\"red green blue\",\"red green blue nir\",\"red green blue cloud-mask red-edge-2 red-edge-3 red-edge-4 nir swir1 swir2\"]\n",
    "bands = types[bandNum]\n",
    "\n",
    "for i in range(imgNum):\n",
    "    img = wf.ImageCollection.from_id(imgColl,start_datetime=img_st[i], end_datetime=img_end[i]).pick_bands(bands)\n",
    "    img = img.filter(lambda img: img.properties[\"cloud_fraction\"] <= cloudFraction)\n",
    "    img_msk = img.map(lambda img: img.mask(img.pick_bands('cloud-mask')==1)) if bandNum > 1 else img\n",
    "    mos = (img_msk.mosaic().pick_bands(\"red green blue\"))\n",
    "    mos.visualize('Image '+str(i+1), map=m)\n",
    "\n",
    "with open(dmgAss) as f: dmg = json.load(f)\n",
    "dmg_geojson = ipyleaflet.GeoJSON(data=dmg,\n",
    "                   style={\"color\": \"red\", \"lineOpacity\": 0.5, \"fillOpacity\": 0.1},\n",
    "                   hover_style={\"fillOpacity\": 0.1})\n",
    "\n",
    "m.add_layer(dmg_geojson)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create .tiff files from Sentinel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate tiff files containing all bands for each tile\n",
    "if newLocation:\n",
    "    for i in range(imgNum):\n",
    "        out = dataPath+locName+\"/imgs_\"+str(i+1)+\"/\"\n",
    "        generate_tiff_from_polygons(dmgAss,\n",
    "                        products=imgColl,\n",
    "                        bands=bands,\n",
    "                        resolution=tifResolution,\n",
    "                        tilesize=tifTilesize,\n",
    "                        pad=tifPad,\n",
    "                        start_datetime=img_st[i],\n",
    "                        end_datetime=img_end[i],\n",
    "                        out_folder=out,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: /home/jovyan/OSCD/new/ochosRios_0/cm/cm.png is a low contrast image\n",
      "  from ipykernel import kernelapp as app\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: /home/jovyan/OSCD/new/ochosRios_1/cm/cm.png is a low contrast image\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Separate bands from tiff files and place in correct folder structure for evaluation\n",
    "if newLocation:    \n",
    "    file = open(dataPath+'test.txt','w') if newTest else open(dataPath+'test.txt','a') # Open test.txt file\n",
    "    bandNumbers = [[4,3,2], [4,3,2,8], [4,3,2,1,5,6,7,8,9,10]] # Set appropriate band numbers for Sentinel bands\n",
    "\n",
    "    for i in range(len([name for name in os.listdir(out) if 'image' in name])): # Loop over tile number from location\n",
    "        imgFolder = dataPath+locName+'_'+str(i) # Create directory for each tile in location\n",
    "        if not os.path.exists(imgFolder): os.makedirs(imgFolder)\n",
    "        file.write(','+locName+'_'+str(i)) if i>0 else file.write(locName+'_'+str(i)) # Write tile number to locations to be tested\n",
    "\n",
    "        if not os.path.exists(imgFolder+\"/cm/\"): os.makedirs(imgFolder+\"/cm/\")\n",
    "        targDest = imgFolder+\"/cm/\"+locName+'_'+str(i)+\"-cm.tif\"\n",
    "        copyfile(dataPath+locName+\"/imgs_1/target_\"+str(i)+\".tiff\", targDest) # Copy target file to tile directory\n",
    "        imgconv = io.imread(targDest)\n",
    "        io.imsave(imgFolder+\"/cm/cm.png\", imgconv)\n",
    "\n",
    "        for j in range(imgNum): # Loop over time points for images\n",
    "            img = io.imread(dataPath+locName+\"/imgs_\"+str(j+1)+\"/\"+'image_'+str(i)+'.tiff')\n",
    "            dest = imgFolder+\"/imgs_\"+str(j+1)\n",
    "            if not os.path.exists(dest): os.makedirs(dest)\n",
    "\n",
    "            for k in range(img.shape[2]): # Loop over bands in each image\n",
    "                io.imsave(dest+\"/B\"+\"{0:0=2d}\".format(bandNumbers[bandNum][k])+\".tif\",img[:,:,k])\n",
    "\n",
    "    file.close()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in new dataset (need to run Change Detection class at bottom of notebook first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ochosRios_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:00<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 1024, 4) (1024, 1024, 4)\n",
      "ochosRios_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 1024, 4) (1024, 1024, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load in new dataset\n",
    "new_dataset = ChangeDetectionDataset(dataPath, train = False, stride = TRAIN_STRIDE)\n",
    "new_loader = DataLoader(new_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aguasclaras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/14 [00:00<00:06,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(471, 525, 4) (471, 525, 4)\n",
      "bercy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 2/14 [00:01<00:06,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(395, 360, 4) (395, 360, 4)\n",
      "bordeaux\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 3/14 [00:01<00:06,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517, 461, 4) (517, 461, 4)\n",
      "nantes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 4/14 [00:02<00:06,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(522, 582, 4) (522, 582, 4)\n",
      "paris\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 5/14 [00:03<00:06,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 390, 4) (408, 390, 4)\n",
      "rennes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 6/14 [00:03<00:05,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 563, 4) (339, 563, 4)\n",
      "saclay_e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 7/14 [00:04<00:04,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(639, 688, 4) (631, 679, 4)\n",
      "abudhabi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 8/14 [00:05<00:04,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(799, 785, 4) (795, 782, 4)\n",
      "cupertino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 9/14 [00:06<00:04,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1015, 788, 4) (1015, 788, 4)\n",
      "pisa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 10/14 [00:07<00:03,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(776, 718, 4) (776, 718, 4)\n",
      "beihai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 11/14 [00:08<00:02,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(902, 772, 4) (902, 772, 4)\n",
      "hongkong\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 12/14 [00:09<00:01,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(695, 540, 4) (693, 538, 4)\n",
      "beirut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 13/14 [00:10<00:01,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1180, 1070, 4) (1180, 1070, 4)\n",
      "mumbai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:11<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(858, 557, 4) (858, 557, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained parameters  1350866\n",
      "LOAD OK\n"
     ]
    }
   ],
   "source": [
    "# Load in pre-trained model\n",
    "train_dataset, weights, net, net_name, criterion, params = init_network(DATA_AUG, PATH_TO_TRAIN, TRAIN_STRIDE, TYPE)\n",
    "print('Trained parameters ', params)\n",
    "\n",
    "if LOAD_TRAINED:\n",
    "    net.load_state_dict(torch.load(modelWeights, map_location=torch.device('cpu')),strict=False) #torch.load('/home/jovyan/gtc-exposure/change_detection/net_final.pth.tar'))\n",
    "    print('LOAD OK')\n",
    "else:\n",
    "    t_start = time.time()\n",
    "    out_dic = train()\n",
    "    t_end = time.time()\n",
    "    print(out_dic)\n",
    "    print('Elapsed time:')\n",
    "    print(t_end - t_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/home/jovyan/gtc-exposure/change_detection/utils_cd.py:232: UserWarning: ./results/FC-EF-ochosRios_0.png is a low contrast image\n",
      "  io.imsave(f'./results/{net_name}-{name}.png',I)\n",
      "Lossy conversion from int64 to uint8. Range [0, 255]. Convert image to uint8 prior to saving to suppress this warning.\n",
      " 50%|█████     | 1/2 [00:04<00:04,  4.98s/it]/home/jovyan/gtc-exposure/change_detection/utils_cd.py:232: UserWarning: ./results/FC-EF-ochosRios_1.png is a low contrast image\n",
      "  io.imsave(f'./results/{net_name}-{name}.png',I)\n",
      "Lossy conversion from int64 to uint8. Range [0, 255]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "100%|██████████| 2/2 [00:08<00:00,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 8.884415864944458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate .png images of detection -> Generates .png results for each tile\n",
    "t_start = time.time()\n",
    "save_test_results(new_dataset, net, net_name)\n",
    "t_end = time.time()\n",
    "print('Elapsed time: {}'.format(t_end - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:40<00:00, 20.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-meaningful results, check out imagery results: {'net_loss': 1.8596951961517334, 'net_accuracy': 63.758182525634766, 'class_accuracy': [71.58448028564453, 15.678466796875], 'precision': 0.08241200451906189, 'recall': 0.15678466930761867, 'dice': 0.10803610826847428, 'kappa': -0.0924464796594512}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Assess accuracy\n",
    "results = test(new_dataset, net, criterion)\n",
    "pprint(results) if gjData else print('Non-meaningful results, check out imagery results:',results)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Detection Dataset class -> could be put in other file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTILS OK\n"
     ]
    }
   ],
   "source": [
    "# Functions\n",
    "\n",
    "def adjust_shape(I, s):\n",
    "    \"\"\"Adjust shape of grayscale image I to s.\"\"\"\n",
    "    \n",
    "    # crop if necesary\n",
    "    I = I[:s[0],:s[1]]\n",
    "    si = I.shape\n",
    "    \n",
    "    # pad if necessary \n",
    "    p0 = max(0,s[0] - si[0])\n",
    "    p1 = max(0,s[1] - si[1])\n",
    "    \n",
    "    return np.pad(I,((0,p0),(0,p1)),'edge')\n",
    "    \n",
    "\n",
    "def read_sentinel_img(path):\n",
    "    \"\"\"Read cropped Sentinel-2 image: RGB bands.\"\"\"\n",
    "    im_name = os.listdir(path)[0][:-7]\n",
    "    r = io.imread(path + im_name + \"B04.tif\")\n",
    "    g = io.imread(path + im_name + \"B03.tif\")\n",
    "    b = io.imread(path + im_name + \"B02.tif\")\n",
    "    \n",
    "    I = np.stack((r,g,b),axis=2).astype('float')\n",
    "    \n",
    "    if NORMALISE_IMGS:\n",
    "        I = (I - I.mean()) / I.std()\n",
    "\n",
    "    return I\n",
    "\n",
    "def read_sentinel_img_4(path):\n",
    "    \"\"\"Read cropped Sentinel-2 image: RGB and NIR bands.\"\"\"\n",
    "    im_name = os.listdir(path)[0][:-7]\n",
    "    r = io.imread(path + im_name + \"B04.tif\")\n",
    "    g = io.imread(path + im_name + \"B03.tif\")\n",
    "    b = io.imread(path + im_name + \"B02.tif\")\n",
    "    nir = io.imread(path + im_name + \"B08.tif\")\n",
    "    \n",
    "    I = np.stack((r,g,b,nir),axis=2).astype('float')\n",
    "    \n",
    "    I = I.reshape(I.shape[0],I.shape[1],I.shape[2])\n",
    "    \n",
    "    if NORMALISE_IMGS:\n",
    "        I = (I - I.mean()) / I.std()\n",
    "\n",
    "    return I\n",
    "\n",
    "def read_sentinel_img_leq20(path):\n",
    "    \"\"\"Read cropped Sentinel-2 image: bands with resolution less than or equals to 20m.\"\"\"\n",
    "    im_name = os.listdir(path)[0][:-7]\n",
    "    \n",
    "    r = io.imread(path + im_name + \"B04.tif\")\n",
    "    s = r.shape\n",
    "    g = io.imread(path + im_name + \"B03.tif\")\n",
    "    b = io.imread(path + im_name + \"B02.tif\")\n",
    "    nir = io.imread(path + im_name + \"B08.tif\")\n",
    "    \n",
    "    ir1 = adjust_shape(zoom(io.imread(path + im_name + \"B05.tif\"),2),s)\n",
    "    ir2 = adjust_shape(zoom(io.imread(path + im_name + \"B06.tif\"),2),s)\n",
    "    ir3 = adjust_shape(zoom(io.imread(path + im_name + \"B07.tif\"),2),s)\n",
    "    nir2 = adjust_shape(zoom(io.imread(path + im_name + \"B8A.tif\"),2),s)\n",
    "    swir2 = adjust_shape(zoom(io.imread(path + im_name + \"B11.tif\"),2),s)\n",
    "    swir3 = adjust_shape(zoom(io.imread(path + im_name + \"B12.tif\"),2),s)\n",
    "    \n",
    "    I = np.stack((r,g,b,nir,ir1,ir2,ir3,nir2,swir2,swir3),axis=2).astype('float')\n",
    "    \n",
    "    if NORMALISE_IMGS:\n",
    "        I = (I - I.mean()) / I.std()\n",
    "\n",
    "    return I\n",
    "\n",
    "def read_sentinel_img_leq60(path):\n",
    "    \"\"\"Read cropped Sentinel-2 image: all bands.\"\"\"\n",
    "    im_name = os.listdir(path)[0][:-7]\n",
    "    \n",
    "    r = io.imread(path + im_name + \"B04.tif\")\n",
    "    s = r.shape\n",
    "    g = io.imread(path + im_name + \"B03.tif\")\n",
    "    b = io.imread(path + im_name + \"B02.tif\")\n",
    "    nir = io.imread(path + im_name + \"B08.tif\")\n",
    "    \n",
    "    ir1 = adjust_shape(zoom(io.imread(path + im_name + \"B05.tif\"),2),s)\n",
    "    ir2 = adjust_shape(zoom(io.imread(path + im_name + \"B06.tif\"),2),s)\n",
    "    ir3 = adjust_shape(zoom(io.imread(path + im_name + \"B07.tif\"),2),s)\n",
    "    nir2 = adjust_shape(zoom(io.imread(path + im_name + \"B8A.tif\"),2),s)\n",
    "    swir2 = adjust_shape(zoom(io.imread(path + im_name + \"B11.tif\"),2),s)\n",
    "    swir3 = adjust_shape(zoom(io.imread(path + im_name + \"B12.tif\"),2),s)\n",
    "    \n",
    "    uv = adjust_shape(zoom(io.imread(path + im_name + \"B01.tif\"),6),s)\n",
    "    wv = adjust_shape(zoom(io.imread(path + im_name + \"B09.tif\"),6),s)\n",
    "    swirc = adjust_shape(zoom(io.imread(path + im_name + \"B10.tif\"),6),s)\n",
    "    \n",
    "    I = np.stack((r,g,b,nir,ir1,ir2,ir3,nir2,swir2,swir3,uv,wv,swirc),axis=2).astype('float')\n",
    "    \n",
    "    if NORMALISE_IMGS:\n",
    "        I = (I - I.mean()) / I.std()\n",
    "\n",
    "    return I\n",
    "\n",
    "def read_sentinel_img_trio(path):\n",
    "    \"\"\"Read cropped Sentinel-2 image pair and change map.\"\"\"\n",
    "#     read images\n",
    "    if TYPE == 0:\n",
    "        I1 = read_sentinel_img(path + '/imgs_1/')\n",
    "        I2 = read_sentinel_img(path + '/imgs_2/')\n",
    "    elif TYPE == 1:\n",
    "        I1 = read_sentinel_img_4(path + '/imgs_1/')\n",
    "        I2 = read_sentinel_img_4(path + '/imgs_2/')\n",
    "    elif TYPE == 2:\n",
    "        I1 = read_sentinel_img_leq20(path + '/imgs_1/')\n",
    "        I2 = read_sentinel_img_leq20(path + '/imgs_2/')\n",
    "    elif TYPE == 3:\n",
    "        I1 = read_sentinel_img_leq60(path + '/imgs_1/')\n",
    "        I2 = read_sentinel_img_leq60(path + '/imgs_2/')\n",
    "        \n",
    "    cm = io.imread(path + '/cm/cm.png', as_gray=True) != 0\n",
    "    \n",
    "    # crop if necessary\n",
    "    s1 = I1.shape\n",
    "    s2 = I2.shape\n",
    "    print(s1,s2)\n",
    "    I2 = np.pad(I2,((0, s1[0] - s2[0]), (0, s1[1] - s2[1]), (0,0)),'edge')\n",
    "    \n",
    "    \n",
    "    return I1, I2, cm\n",
    "\n",
    "\n",
    "\n",
    "def reshape_for_torch(I):\n",
    "    \"\"\"Transpose image for PyTorch coordinates.\"\"\"\n",
    "#     out = np.swapaxes(I,1,2)\n",
    "#     out = np.swapaxes(out,0,1)\n",
    "#     out = out[np.newaxis,:]\n",
    "    out = I.transpose((2, 0, 1))\n",
    "    return torch.from_numpy(out)\n",
    "\n",
    "\n",
    "\n",
    "class ChangeDetectionDataset(Dataset):\n",
    "    \"\"\"Change Detection dataset class, used for both training and test data.\"\"\"\n",
    "\n",
    "    def __init__(self, path, train = True, patch_side = 96, stride = None, use_all_bands = False, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        # basics\n",
    "        self.transform = transform\n",
    "        self.path = path\n",
    "        self.patch_side = patch_side\n",
    "        if not stride:\n",
    "            self.stride = 1\n",
    "        else:\n",
    "            self.stride = stride\n",
    "        \n",
    "        if train:\n",
    "            fname = 'train.txt'\n",
    "        else:\n",
    "            fname = 'test.txt'\n",
    "        \n",
    "#         print(path + fname)\n",
    "        self.names = read_csv(path + fname).columns\n",
    "        self.n_imgs = self.names.shape[0]\n",
    "        \n",
    "        n_pix = 0\n",
    "        true_pix = 0\n",
    "        \n",
    "        \n",
    "        # load images\n",
    "        self.imgs_1 = {}\n",
    "        self.imgs_2 = {}\n",
    "        self.change_maps = {}\n",
    "        self.n_patches_per_image = {}\n",
    "        self.n_patches = 0\n",
    "        self.patch_coords = []\n",
    "        for im_name in tqdm(self.names):\n",
    "            # load and store each image\n",
    "            print(im_name)\n",
    "            I1, I2, cm = read_sentinel_img_trio(self.path + im_name)\n",
    "            self.imgs_1[im_name] = reshape_for_torch(I1)\n",
    "            self.imgs_2[im_name] = reshape_for_torch(I2)\n",
    "            self.change_maps[im_name] = cm\n",
    "            \n",
    "            s = cm.shape\n",
    "            n_pix += np.prod(s)\n",
    "            true_pix += cm.sum()\n",
    "            \n",
    "            # calculate the number of patches\n",
    "            s = self.imgs_1[im_name].shape\n",
    "            n1 = ceil((s[1] - self.patch_side + 1) / self.stride)\n",
    "            n2 = ceil((s[2] - self.patch_side + 1) / self.stride)\n",
    "            n_patches_i = n1 * n2\n",
    "            self.n_patches_per_image[im_name] = n_patches_i\n",
    "            self.n_patches += n_patches_i\n",
    "            \n",
    "            # generate path coordinates\n",
    "            for i in range(n1):\n",
    "                for j in range(n2):\n",
    "                    # coordinates in (x1, x2, y1, y2)\n",
    "                    current_patch_coords = (im_name, \n",
    "                                    [self.stride*i, self.stride*i + self.patch_side, self.stride*j, self.stride*j + self.patch_side],\n",
    "                                    [self.stride*(i + 1), self.stride*(j + 1)])\n",
    "                    self.patch_coords.append(current_patch_coords)\n",
    "                    \n",
    "        self.weights = [ FP_MODIFIER * 2 * true_pix / n_pix, 2 * (n_pix - true_pix) / n_pix]   \n",
    "\n",
    "    def get_img(self, im_name):\n",
    "        return self.imgs_1[im_name], self.imgs_2[im_name], self.change_maps[im_name]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_patches\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_patch_coords = self.patch_coords[idx]\n",
    "        im_name = current_patch_coords[0]\n",
    "        limits = current_patch_coords[1]\n",
    "        centre = current_patch_coords[2]\n",
    "        \n",
    "        I1 = self.imgs_1[im_name][:, limits[0]:limits[1], limits[2]:limits[3]]\n",
    "        I2 = self.imgs_2[im_name][:, limits[0]:limits[1], limits[2]:limits[3]]\n",
    "        \n",
    "        label = self.change_maps[im_name][limits[0]:limits[1], limits[2]:limits[3]]\n",
    "        label = torch.from_numpy(1*np.array(label)).float()\n",
    "        \n",
    "        sample = {'I1': I1, 'I2': I2, 'label': label}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "class RandomFlip(object):\n",
    "    \"\"\"Flip randomly the images in a sample.\"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "#         return\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        I1, I2, label = sample['I1'], sample['I2'], sample['label']\n",
    "        \n",
    "        if random.random() > 0.5:\n",
    "            I1 =  I1.numpy()[:,:,::-1].copy()\n",
    "            I1 = torch.from_numpy(I1)\n",
    "            I2 =  I2.numpy()[:,:,::-1].copy()\n",
    "            I2 = torch.from_numpy(I2)\n",
    "            label =  label.numpy()[:,::-1].copy()\n",
    "            label = torch.from_numpy(label)\n",
    "\n",
    "        return {'I1': I1, 'I2': I2, 'label': label}\n",
    "\n",
    "class RandomRot(object):\n",
    "    \"\"\"Rotate randomly the images in a sample.\"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "#         return\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        I1, I2, label = sample['I1'], sample['I2'], sample['label']\n",
    "        \n",
    "        n = random.randint(0, 3)\n",
    "        if n:\n",
    "            I1 =  sample['I1'].numpy()\n",
    "            I1 = np.rot90(I1, n, axes=(1, 2)).copy()\n",
    "            I1 = torch.from_numpy(I1)\n",
    "            I2 =  sample['I2'].numpy()\n",
    "            I2 = np.rot90(I2, n, axes=(1, 2)).copy()\n",
    "            I2 = torch.from_numpy(I2)\n",
    "            label =  sample['label'].numpy()\n",
    "            label = np.rot90(label, n, axes=(0, 1)).copy()\n",
    "            label = torch.from_numpy(label)\n",
    "\n",
    "        return {'I1': I1, 'I2': I2, 'label': label}\n",
    "\n",
    "\"\"\"\n",
    "Network initialisation function\n",
    "\"\"\"\n",
    "# Initialise network\n",
    "def init_network(DATA_AUG, PATH_TO_TRAIN, TRAIN_STRIDE, TYPE):\n",
    "    if DATA_AUG:\n",
    "        data_transform = tr.Compose([RandomFlip(), RandomRot()])\n",
    "    else:\n",
    "        data_transform = None\n",
    "    train_dataset = ChangeDetectionDataset(PATH_TO_TRAIN, train = True, stride = TRAIN_STRIDE, transform=data_transform)\n",
    "    weights = torch.FloatTensor(train_dataset.weights)\n",
    "\n",
    "    # 0-RGB | 1-RGBIr | 2-All bands s.t. resulution <= 20m | 3-All bands\n",
    "\n",
    "    if TYPE == 0:\n",
    "        net, net_name = Unet(2*3, 2), 'FC-EF'\n",
    "    #     net, net_name = Unet(2*3, 2), 'FC-EF'\n",
    "    #     net, net_name = SiamUnet_conc(3, 2), 'FC-Siam-conc'\n",
    "    #     net, net_name = SiamUnet_diff(3, 2), 'FC-Siam-diff'#\n",
    "    #     net, net_name = FresUNet(2*3, 2), 'FresUNet'\n",
    "    elif TYPE == 1:\n",
    "    #     net, net_name = SmallUnet(2*4, 2), 'FC-EF'\n",
    "        net, net_name = Unet(2*4, 2), 'FC-EF'\n",
    "    #     net, net_name = SiamUnet_conc(4, 2), 'FC-Siam-conc'\n",
    "    #     net, net_name = SiamUnet_diff(4, 2), 'FC-Siam-diff'\n",
    "    #     net, net_name = FresUNet(2*4, 2), 'FresUNet'\n",
    "    elif TYPE == 2:\n",
    "        net, net_name = SmallUnet(2*10, 2), 'FC-EF'\n",
    "    #     net, net_name = Unet(2*10, 2), 'FC-EF'\n",
    "    #     net, net_name = SiamUnet_conc(10, 2), 'FC-Siam-conc'\n",
    "    #     net, net_name = SiamUnet_diff(10, 2), 'FC-Siam-diff'\n",
    "    #     net, net_name = FresUNet(2*10, 2), 'FresUNet'\n",
    "    elif TYPE == 3:\n",
    "        net, net_name = SmallUnet(2*13, 2), 'FC-EF'\n",
    "    #     net, net_name = Unet(2*13, 2), 'FC-EF'\n",
    "    #     net, net_name = SiamUnet_conc(13, 2), 'FC-Siam-conc'\n",
    "    #     net, net_name = SiamUnet_diff(13, 2), 'FC-Siam-diff'\n",
    "    #     net, net_name = FresUNet(2*13, 2), 'FresUNet'\n",
    "\n",
    "    criterion = nn.NLLLoss(weight=weights) # to be used with logsoftmax output\n",
    "    params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    \n",
    "    return train_dataset, weights, net, net_name, criterion, params\n",
    "    \n",
    "print('UTILS OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "Fully Convolutional Networks for Change Detection code for training the network presented in the paper:\n",
    "\n",
    "```\n",
    "Daudt, R.C., Le Saux, B. and Boulch, A., 2018, October. Fully convolutional siamese networks for change detection. In 2018 25th IEEE International Conference on Image Processing (ICIP) (pp. 4063-4067). IEEE.\n",
    "```\n",
    "\n",
    "Code uses the OSCD dataset:\n",
    "\n",
    "```\n",
    "Daudt, R.C., Le Saux, B., Boulch, A. and Gousseau, Y., 2018, July. Urban change detection for multispectral earth observation using convolutional neural networks. In IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium (pp. 2115-2118). IEEE.\n",
    "```\n",
    "\n",
    "\n",
    "FresUNet architecture from paper:\n",
    "\n",
    "```\n",
    "Daudt, R.C., Le Saux, B., Boulch, A. and Gousseau, Y., 2019. Multitask learning for large-scale semantic change detection. Computer Vision and Image Understanding, 187, p.102783.\n",
    "```\n",
    "\n",
    "Please consider all relevant papers if you use this code.\n",
    "\n",
    "Rodrigo Daudt\n",
    "rcdaudt.github.io\n",
    "rodrigo.daudt@onera.fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
